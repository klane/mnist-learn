{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "tfl = tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim, dropout=0.4, relu_alpha=0.2, momentum=0.9, initializer=None):\n",
    "    model = Sequential(name='Generator')\n",
    "\n",
    "    shape = (7, 7, 128)\n",
    "    model.add(tfl.Dense(np.prod(shape), kernel_initializer=initializer, input_dim=latent_dim))\n",
    "    model.add(tfl.BatchNormalization(momentum=momentum))\n",
    "    model.add(tfl.LeakyReLU(alpha=relu_alpha))\n",
    "    model.add(tfl.Reshape(shape))\n",
    "    model.add(tfl.Dropout(dropout))\n",
    "\n",
    "    model.add(tfl.Conv2DTranspose(128, kernel_size=5, strides=2, padding='same', kernel_initializer=initializer))\n",
    "    model.add(tfl.BatchNormalization(momentum=momentum))\n",
    "    model.add(tfl.LeakyReLU(alpha=relu_alpha))\n",
    "\n",
    "    model.add(tfl.Conv2DTranspose(64, kernel_size=5, strides=2, padding='same', kernel_initializer=initializer))\n",
    "    model.add(tfl.BatchNormalization(momentum=momentum))\n",
    "    model.add(tfl.LeakyReLU(alpha=relu_alpha))\n",
    "\n",
    "    model.add(tfl.Conv2DTranspose(32, kernel_size=5, padding='same', kernel_initializer=initializer))\n",
    "    model.add(tfl.BatchNormalization(momentum=momentum))\n",
    "    model.add(tfl.LeakyReLU(alpha=relu_alpha))\n",
    "\n",
    "    model.add(tfl.Conv2D(1, kernel_size=5, activation='tanh', padding='same', kernel_initializer=initializer))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(image_shape, dropout=0.4, relu_alpha=0.2, initializer=None):\n",
    "    model = Sequential(name='Discriminator')\n",
    "\n",
    "    model.add(tfl.Conv2D(32, kernel_size=5, strides=2, padding='same', kernel_initializer=initializer, input_shape=image_shape))\n",
    "    model.add(tfl.LeakyReLU(alpha=relu_alpha))\n",
    "\n",
    "    model.add(tfl.Conv2D(64, kernel_size=5, strides=2, padding='same', kernel_initializer=initializer))\n",
    "    model.add(tfl.LeakyReLU(alpha=relu_alpha))\n",
    "    model.add(tfl.Dropout(dropout))\n",
    "\n",
    "    model.add(tfl.Conv2D(128, kernel_size=5, strides=2, padding='same', kernel_initializer=initializer))\n",
    "    model.add(tfl.LeakyReLU(alpha=relu_alpha))\n",
    "    model.add(tfl.Dropout(dropout))\n",
    "\n",
    "    model.add(tfl.Conv2D(256, kernel_size=5, padding='same', kernel_initializer=initializer))\n",
    "    model.add(tfl.LeakyReLU(alpha=relu_alpha))\n",
    "    model.add(tfl.Dropout(dropout))\n",
    "    model.add(tfl.Flatten())\n",
    "\n",
    "    model.add(tfl.Dense(1, activation='sigmoid', kernel_initializer=initializer))\n",
    "\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    model = Sequential(name='GAN')\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    \n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    (X_train, _), (X_test, _) = mnist.load_data()\n",
    "    \n",
    "    # stack train and test images\n",
    "    X = np.vstack((X_train, X_test))\n",
    "    \n",
    "    # add channel to images (required by tensorflow)\n",
    "    X = np.expand_dims(X, axis=-1)\n",
    "    \n",
    "    # convert images to floats\n",
    "    X = X.astype('float32')\n",
    "    \n",
    "    # scale images to [-1,1]\n",
    "    X = X / 127.5 - 1\n",
    "    \n",
    "    return X\n",
    "\n",
    "# sample from latent space\n",
    "def generate_latent_data(latent_dim, n_samples):\n",
    "    return np.random.normal(0, 1, (n_samples, latent_dim))\n",
    "\n",
    "# generate real data to train discriminator\n",
    "def generate_real_data(data, n_samples):\n",
    "    index = np.random.randint(0, data.shape[0], n_samples)\n",
    "    X = data[index]\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "# generate fake data to train discriminator\n",
    "def generate_fake_data(generator, latent_dim, n_samples):\n",
    "    latent = generate_latent_data(latent_dim, n_samples)\n",
    "    X = generator.predict(latent)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "# plot set of images at the current\n",
    "def save_images(images, image_name, rows, cols):\n",
    "    # scale images to [0,1]\n",
    "    images = (images + 1) / 2.0\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            axs[i,j].imshow(images[j + i*cols, :, :, 0], cmap='gray_r')\n",
    "            axs[i,j].axis('off')\n",
    "\n",
    "    fig.savefig(f'{image_name}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, gan, data, latent_dim, soft_label=0.9, pct_wrong=0.05,\n",
    "          epochs=1000, discriminator_batch_size=128, generator_batch_size=128,\n",
    "          print_interval=10, save_interval=50, image_dir='images', image_rows=5, image_cols=5):\n",
    "    # discriminator batch size for real and fake data\n",
    "    half_batch = int(discriminator_batch_size / 2)\n",
    "    \n",
    "    # samples from latent space used to plot generator evolution\n",
    "    latent = generate_latent_data(latent_dim, image_rows * image_cols)\n",
    "    \n",
    "    for epoch in range(epochs+1):\n",
    "        # select random batch of images\n",
    "        X_real, y_real = generate_real_data(data, half_batch)\n",
    "\n",
    "        # generate batch of fake images\n",
    "        X_fake, y_fake = generate_fake_data(generator, latent_dim, half_batch)\n",
    "        \n",
    "        # add percentage of wrong data to discriminator training data\n",
    "        if pct_wrong > 0:\n",
    "            num_wrong = int(pct_wrong * half_batch)\n",
    "            \n",
    "            # add fake data to real training data\n",
    "            X, y = generate_fake_data(generator, latent_dim, num_wrong)\n",
    "            X_real, y_real = np.vstack((X_real, X)), np.vstack((y_real, 1 - y))\n",
    "            \n",
    "            # add real data to fake training data\n",
    "            X, y = generate_real_data(data, num_wrong)\n",
    "            X_fake, y_fake = np.vstack((X_fake, X)), np.vstack((y_fake, 1 - y))\n",
    "        \n",
    "        # train discriminator\n",
    "        d_loss_real, acc_real = discriminator.train_on_batch(X_real, y_real * soft_label)\n",
    "        d_loss_fake, acc_fake = discriminator.train_on_batch(X_fake, y_fake)\n",
    "        \n",
    "        # evaluate real data without soft label\n",
    "        if soft_label != 1:\n",
    "            d_loss_real, acc_real = discriminator.evaluate(X_real, y_real, verbose=0)\n",
    "        \n",
    "        d_loss = (d_loss_real + d_loss_fake) / 2.0\n",
    "\n",
    "        # train generator\n",
    "        X_gan = generate_latent_data(latent_dim, generator_batch_size)\n",
    "        y_gan = np.ones((generator_batch_size, 1))\n",
    "        g_loss = gan.train_on_batch(X_gan, y_gan)\n",
    "\n",
    "        # print progress\n",
    "        if epoch % print_interval == 0:\n",
    "            e = f'{epoch}/{epochs}'\n",
    "            print(f'epoch {e}, D: [loss={d_loss:.3f}, real={acc_real*100:.2f}, fake={acc_fake*100:.2f}], G: [loss={g_loss:.3f}]')\n",
    "\n",
    "        # save images generated from latent space\n",
    "        if epoch % save_interval == 0:\n",
    "            images = generator.predict(latent)\n",
    "            image_name = f'{image_dir}/mnist_epoch_{epoch}'\n",
    "            save_images(images, image_name, image_rows, image_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rows = 28\n",
    "image_cols = 28\n",
    "image_shape = (image_rows, image_cols, 1)\n",
    "latent_dim = 100\n",
    "initializer = RandomNormal(mean=0.0, stddev=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator(latent_dim, initializer=initializer)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator(image_shape, initializer=initializer)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = build_gan(generator, discriminator)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'images'\n",
    "%mkdir -p \"$image_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    generator, discriminator, gan, data, latent_dim, image_dir=image_dir,\n",
    "    epochs=1000, discriminator_batch_size=128, generator_batch_size=200\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
